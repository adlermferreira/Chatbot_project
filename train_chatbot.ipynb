{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP/h70y6BMJ1DWr8ClNjaUg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adlermferreira/Chatbot_project/blob/master/train_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqxnUuiYi14Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c25d31f2-540d-48fc-8a93-9bdec7d4ebe6"
      },
      "source": [
        "# This cell imports the drive library and mounts your Google Drive as a VM local drive. You can access to your Drive files \r\n",
        "# using this path \"/content/gdrive/My Drive/\"\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_L118V-Gesju"
      },
      "source": [
        "path = \"/content/gdrive/My Drive/Colab Notebooks/Data Science/\""
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iEV6f4idsPa"
      },
      "source": [
        "import numpy as np\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Activation, Dropout\r\n",
        "from keras.optimizers import SGD\r\n",
        "import random\r\n",
        "import nltk\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "lemmatizer = WordNetLemmatizer()\r\n",
        "import json\r\n",
        "import pickle\r\n",
        "intents_file = open(path + '/intents.json').read()\r\n",
        "intents = json.loads(intents_file)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XV5wJwZuqpeB",
        "outputId": "d1ed6093-0983-4af0-b87f-798bb894fb7e"
      },
      "source": [
        "nltk.download('punkt')\r\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fih65h99b0S-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b9509b0-7512-48f4-bbe9-99a1a3013d1a"
      },
      "source": [
        "words = []\r\n",
        "classes = []\r\n",
        "documents = []\r\n",
        "ignore_letters = ['!', '?', ',', '.']\r\n",
        "\r\n",
        "for intent in intents['intents']:\r\n",
        "    for pattern in intent['patterns']:\r\n",
        "        word = nltk.word_tokenize(pattern)\r\n",
        "        words.extend(word)\r\n",
        "\r\n",
        "        documents.append((word, intent['tag']))\r\n",
        "        if intent['tag'] not in classes:\r\n",
        "            classes.append(intent['tag'])\r\n",
        "\r\n",
        "print(documents)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(['Hi', 'there'], 'greeting'), (['How', 'are', 'you'], 'greeting'), (['Is', 'anyone', 'there', '?'], 'greeting'), (['Hey'], 'greeting'), (['Hola'], 'greeting'), (['Hello'], 'greeting'), (['Good', 'day'], 'greeting'), (['Bye'], 'goodbye'), (['See', 'you', 'later'], 'goodbye'), (['Goodbye'], 'goodbye'), (['Nice', 'chatting', 'to', 'you', ',', 'bye'], 'goodbye'), (['Till', 'next', 'time'], 'goodbye'), (['Thanks'], 'thanks'), (['Thank', 'you'], 'thanks'), (['That', \"'s\", 'helpful'], 'thanks'), (['Awesome', ',', 'thanks'], 'thanks'), (['Thanks', 'for', 'helping', 'me'], 'thanks'), (['How', 'you', 'could', 'help', 'me', '?'], 'options'), (['What', 'you', 'can', 'do', '?'], 'options'), (['What', 'help', 'you', 'provide', '?'], 'options'), (['How', 'you', 'can', 'be', 'helpful', '?'], 'options'), (['What', 'support', 'is', 'offered'], 'options'), (['How', 'to', 'check', 'Adverse', 'drug', 'reaction', '?'], 'adverse_drug'), (['Open', 'adverse', 'drugs', 'module'], 'adverse_drug'), (['Give', 'me', 'a', 'list', 'of', 'drugs', 'causing', 'adverse', 'behavior'], 'adverse_drug'), (['List', 'all', 'drugs', 'suitable', 'for', 'patient', 'with', 'adverse', 'reaction'], 'adverse_drug'), (['Which', 'drugs', 'dont', 'have', 'adverse', 'reaction', '?'], 'adverse_drug'), (['Open', 'blood', 'pressure', 'module'], 'blood_pressure'), (['Task', 'related', 'to', 'blood', 'pressure'], 'blood_pressure'), (['Blood', 'pressure', 'data', 'entry'], 'blood_pressure'), (['I', 'want', 'to', 'log', 'blood', 'pressure', 'results'], 'blood_pressure'), (['Blood', 'pressure', 'data', 'management'], 'blood_pressure'), (['I', 'want', 'to', 'search', 'for', 'blood', 'pressure', 'result', 'history'], 'blood_pressure_search'), (['Blood', 'pressure', 'for', 'patient'], 'blood_pressure_search'), (['Load', 'patient', 'blood', 'pressure', 'result'], 'blood_pressure_search'), (['Show', 'blood', 'pressure', 'results', 'for', 'patient'], 'blood_pressure_search'), (['Find', 'blood', 'pressure', 'results', 'by', 'ID'], 'blood_pressure_search'), (['Find', 'me', 'a', 'pharmacy'], 'pharmacy_search'), (['Find', 'pharmacy'], 'pharmacy_search'), (['List', 'of', 'pharmacies', 'nearby'], 'pharmacy_search'), (['Locate', 'pharmacy'], 'pharmacy_search'), (['Search', 'pharmacy'], 'pharmacy_search'), (['Lookup', 'for', 'hospital'], 'hospital_search'), (['Searching', 'for', 'hospital', 'to', 'transfer', 'patient'], 'hospital_search'), (['I', 'want', 'to', 'search', 'hospital', 'data'], 'hospital_search'), (['Hospital', 'lookup', 'for', 'patient'], 'hospital_search'), (['Looking', 'up', 'hospital', 'details'], 'hospital_search')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiHlesL3rMaj",
        "outputId": "6734b173-88e9-4d06-8576-4d0151b5f241"
      },
      "source": [
        "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_letters]\r\n",
        "words = sorted(list(set(words)))\r\n",
        "\r\n",
        "classes = sorted(list(set(classes)))\r\n",
        "\r\n",
        "print(len(documents), 'documents')\r\n",
        "print(len(classes), 'classes', classes)\r\n",
        "print(len(words), 'unique lemmatize words', words)\r\n",
        "\r\n",
        "pickle.dump(words, open(path + '/words.pkl', 'wb'))\r\n",
        "pickle.dump(classes, open(path + '/classes.pkl', 'wb'))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "47 documents\n",
            "9 classes ['adverse_drug', 'blood_pressure', 'blood_pressure_search', 'goodbye', 'greeting', 'hospital_search', 'options', 'pharmacy_search', 'thanks']\n",
            "87 unique lemmatize words [\"'s\", 'a', 'adverse', 'all', 'anyone', 'are', 'awesome', 'be', 'behavior', 'blood', 'by', 'bye', 'can', 'causing', 'chatting', 'check', 'could', 'data', 'day', 'detail', 'do', 'dont', 'drug', 'entry', 'find', 'for', 'give', 'good', 'goodbye', 'have', 'hello', 'help', 'helpful', 'helping', 'hey', 'hi', 'history', 'hola', 'hospital', 'how', 'i', 'id', 'is', 'later', 'list', 'load', 'locate', 'log', 'looking', 'lookup', 'management', 'me', 'module', 'nearby', 'next', 'nice', 'of', 'offered', 'open', 'patient', 'pharmacy', 'pressure', 'provide', 'reaction', 'related', 'result', 'search', 'searching', 'see', 'show', 'suitable', 'support', 'task', 'thank', 'thanks', 'that', 'there', 'till', 'time', 'to', 'transfer', 'up', 'want', 'what', 'which', 'with', 'you']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07se2_bdyXxO",
        "outputId": "4a0c7ce8-d521-4dcd-c71e-cb6d6c2ba395"
      },
      "source": [
        "training = []\r\n",
        "output_empty = [0] * len(classes)\r\n",
        "\r\n",
        "for doc in documents:\r\n",
        "    bag = []\r\n",
        "\r\n",
        "    word_patterns = doc[0]\r\n",
        "    word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]\r\n",
        "\r\n",
        "    for word in words:\r\n",
        "        bag.append(1) if word in word_patterns else bag.append(0)\r\n",
        "    \r\n",
        "    output_row = list(output_empty)\r\n",
        "    output_row[classes.index(doc[1])] = 1\r\n",
        "    training.append([bag, output_row])\r\n",
        "\r\n",
        "random.shuffle(training)\r\n",
        "training = np.array(training)\r\n",
        "\r\n",
        "train_x = list(training[:, 0])\r\n",
        "train_y = list(training[:, 1])\r\n",
        "print(\"Training Data is created\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Data is created\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}